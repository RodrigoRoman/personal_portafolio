{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "759SG4TyfbUn"
      },
      "source": [
        "<h1>Regular Expressions for Corpus Preprocessing\n",
        "</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p>Regular expressions, often abbreviated as regex, are a foundational tool in text analysis and natural language processing (NLP). They provide a robust framework for identifying, extracting, and manipulating patterns within text data, making them essential for preprocessing large corpora. Corpus preprocessing involves various tasks designed to clean and standardize text data, which in turn facilitates more effective data analysis and model training.</p>\n",
        "<p>This guide will explore how regular expressions can be effectively utilized to streamline these preprocessing tasks. From simple operations like searching and replacing text to more complex pattern recognition, regular expressions offer a precise method to handle diverse text processing challenges, ultimately enhancing the accuracy and efficiency of NLP applications.</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ue1YAKx3XDo"
      },
      "source": [
        "The text file contains English-language reviews of food services from the Yelp website: https://www.yelp.com/.\n",
        "\n",
        "It includes one thousand reviews and is part of a dataset found in the UCI Machine Learning Repository, titled \"Sentiment Labelled Sentences\": https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences#."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zj-h4drXD-X9"
      },
      "source": [
        "<h2>Part 1. Load the Data.</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY6yifxscfrx"
      },
      "source": [
        "We load the data from the specified file and obtain a list of 1000 strings/comments.\n",
        "\n",
        "For now, we only require the Numpy and re libraries for managing arrays and regular expressions in Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OJ26dAfhdFnf"
      },
      "outputs": [],
      "source": [
        "import numpy as np    # import Numpy for array handling.\n",
        "import re             # import re for regular expressions handling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QHUmJyjDdGNP"
      },
      "outputs": [],
      "source": [
        "# Execute the following instructions to load the information from the given file:\n",
        "\n",
        "with open('text_corpus_food_reviews.txt',        # you can update the path to your file, if necessary.\n",
        "          mode='r',     # open the file in read mode.\n",
        "          ) as f:\n",
        "    docs = f.readlines()    # separate each comment by lines\n",
        "\n",
        "f.close()  # now that we have the information in the docs variable, close the file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "L6WzrSrodG-Y"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(docs) == list   # Verify that variable \"docs\" is a list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QIK1u9WS2FtS"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(docs)==1000  # verify that the length of \"docs\" is 10000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9AMLIfQvJqNZ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Wow... Loved this place.\\n',\n",
              " 'Crust is not good.\\n',\n",
              " 'Not tasty and the texture was just nasty.\\n',\n",
              " 'Stopped by during the late May bank holiday off Rick Steve recommendation and loved it.\\n',\n",
              " 'The selection on the menu was great and so were the prices.\\n',\n",
              " 'Now I am getting angry and I want my damn pho.\\n',\n",
              " \"Honeslty it didn't taste THAT fresh.)\\n\",\n",
              " 'The potatoes were like rubber and you could tell they had been made up ahead of time being kept under a warmer.\\n',\n",
              " 'The fries were great too.\\n',\n",
              " 'A great touch.\\n']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs[0:10]     # Lets see the first 10 comments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h3>Auxiliar function to print list elements</h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to print elements of a list formatted and numbered\n",
        "def print_elements(list_items):\n",
        "    \"\"\"\n",
        "    Prints each element of the list, aligned in a column, with the index on the left.\n",
        "    \n",
        "    Args:\n",
        "    list_items (list): A list of strings.\n",
        "    \n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    # Get the length of the largest number\n",
        "    max_length = len(str(len(list_items)))\n",
        "    \n",
        "    # Print each element with the index aligned\n",
        "    for index, item in enumerate(list_items, start=1):\n",
        "        print(f\"{index:>{max_length}}. {item}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h2>Problem 1</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78nJMemzn5a5"
      },
      "source": [
        "\n",
        "Search for and remove all newline characters '\\n' at the end of each comment.\n",
        "\n",
        "Once completed, print the first 10 comments from the resulting output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "PwbYYIuZn8pE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 10 comments from the result:\n",
            " 1. Wow... Loved this place.\n",
            " 2. Crust is not good.\n",
            " 3. Not tasty and the texture was just nasty.\n",
            " 4. Stopped by during the late May bank holiday off Rick Steve recommendation and loved it.\n",
            " 5. The selection on the menu was great and so were the prices.\n",
            " 6. Now I am getting angry and I want my damn pho.\n",
            " 7. Honeslty it didn't taste THAT fresh.)\n",
            " 8. The potatoes were like rubber and you could tell they had been made up ahead of time being kept under a warmer.\n",
            " 9. The fries were great too.\n",
            "10. A great touch.\n",
            "Total number of strings found: 1000\n"
          ]
        }
      ],
      "source": [
        "Q = []\n",
        "for doc in docs:\n",
        "  tmp = re.sub(r'\\n', \"\", doc)  \n",
        "  if tmp:\n",
        "    Q.append(tmp)\n",
        "\n",
        "print('First 10 comments from the result:')\n",
        "print_elements(Q[:10])\n",
        "print('Total number of strings found:', len(Q))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h2>Problem 2</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWeKQC93ctEo"
      },
      "source": [
        "\n",
        "Search for and print all words that end with two or more exclamation marks in a row, for example \"!!!\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to print elements of a list formatted, numbered, and with the count of exclamation marks\n",
        "def print_exclamations(list_items):\n",
        "    \"\"\"\n",
        "    Prints each element of the list, aligned in a column, with the index on the left.\n",
        "    \n",
        "    Args:\n",
        "    list_items (list): A list of strings.\n",
        "    \n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    # Maximum length of the indices and elements for formatting\n",
        "    max_length_index = max(len(str(len(list_items))), len(\"Index\"))\n",
        "    max_length_element = max(len(item) for item in list_items)\n",
        "    \n",
        "    # Column headers\n",
        "    print(f\"{'Index':>{max_length_index}} | {'Element':<{max_length_element}} | {'Exclamations':>2}\")\n",
        "    print('-' * (max_length_index + max_length_element + 15))  # 15 for the spaces and the title of the exclamations column\n",
        "    \n",
        "    # Print each element with the index and the number of exclamation marks in the word\n",
        "    for index, element in enumerate(list_items, start=1):\n",
        "        num_exclamations = element.count('!')  # Count how many exclamation marks are in the element\n",
        "        # Adjust the element by removing the exclamation marks for printing\n",
        "        print(f\"{index:>{max_length_index}} | {element:<{max_length_element}} | {num_exclamations:>2}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0p3kMXfddICc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Words with more than two exclamation marks:\n",
            "Index | Element                    | Exclamations\n",
            "----------------------------------------------\n",
            "    1 | Firehouse!!!!!             |  5\n",
            "    2 | APPETIZERS!!!              |  3\n",
            "    3 | amazing!!!                 |  3\n",
            "    4 | buffet!!!                  |  3\n",
            "    5 | good!!                     |  2\n",
            "    6 | it!!!!                     |  4\n",
            "    7 | DELICIOUS!!                |  2\n",
            "    8 | amazing!!                  |  2\n",
            "    9 | shawarrrrrrma!!!!!!        |  6\n",
            "   10 | yucky!!!                   |  3\n",
            "   11 | steak!!!!!                 |  5\n",
            "   12 | delicious!!!               |  3\n",
            "   13 | far!!                      |  2\n",
            "   14 | biscuits!!!                |  3\n",
            "   15 | dry!!                      |  2\n",
            "   16 | disappointing!!!           |  3\n",
            "   17 | awesome!!                  |  2\n",
            "   18 | Up!!                       |  2\n",
            "   19 | FLY!!!!!!!!                |  8\n",
            "   20 | here!!!                    |  3\n",
            "   21 | great!!!!!!!!!!!!!!        | 14\n",
            "   22 | packed!!                   |  2\n",
            "   23 | otherwise!!                |  2\n",
            "   24 | amazing!!!!!!!!!!!!!!!!!!! | 19\n",
            "   25 | style!!                    |  2\n",
            "   26 | disappointed!!             |  2\n",
            "Total number of strings found: 26\n"
          ]
        }
      ],
      "source": [
        "Q = []\n",
        "for doc in docs:\n",
        "  tmp = re.findall(r'\\b\\w+(?:!!+)', doc)  \n",
        "  if tmp:\n",
        "    Q = Q + tmp\n",
        "\n",
        "print('Words with more than two exclamation marks:')\n",
        "print_exclamations(Q)\n",
        "print('Total number of strings found:', len(Q))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h2>Problem 3</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-s3okBqL96TT"
      },
      "source": [
        "\n",
        "Search for and print all words that are written entirely in uppercase. Each match should be a single word.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "yKHJkZKo_nW5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 10 comments from the result:\n",
            "  1. I\n",
            "  2. I\n",
            "  3. THAT\n",
            "  4. A\n",
            "  5. I\n",
            "  6. I\n",
            "  7. I\n",
            "  8. I\n",
            "  9. I\n",
            " 10. I\n",
            " 11. I\n",
            " 12. I\n",
            " 13. I\n",
            " 14. I\n",
            " 15. I\n",
            " 16. I\n",
            " 17. APPETIZERS\n",
            " 18. A\n",
            " 19. I\n",
            " 20. I\n",
            " 21. I\n",
            " 22. I\n",
            " 23. I\n",
            " 24. I\n",
            " 25. I\n",
            " 26. WILL\n",
            " 27. NEVER\n",
            " 28. EVER\n",
            " 29. STEP\n",
            " 30. FORWARD\n",
            " 31. IN\n",
            " 32. IT\n",
            " 33. AGAIN\n",
            " 34. I\n",
            " 35. LOVED\n",
            " 36. I\n",
            " 37. AND\n",
            " 38. REAL\n",
            " 39. I\n",
            " 40. I\n",
            " 41. I\n",
            " 42. I\n",
            " 43. BITCHES\n",
            " 44. I\n",
            " 45. I\n",
            " 46. I\n",
            " 47. I\n",
            " 48. NYC\n",
            " 49. I\n",
            " 50. I\n",
            " 51. I\n",
            " 52. I\n",
            " 53. I\n",
            " 54. I\n",
            " 55. I\n",
            " 56. STALE\n",
            " 57. I\n",
            " 58. I\n",
            " 59. DELICIOUS\n",
            " 60. I\n",
            " 61. I\n",
            " 62. I\n",
            " 63. I\n",
            " 64. I\n",
            " 65. I\n",
            " 66. I\n",
            " 67. I\n",
            " 68. I\n",
            " 69. I\n",
            " 70. I\n",
            " 71. I\n",
            " 72. I\n",
            " 73. I\n",
            " 74. I\n",
            " 75. WORST\n",
            " 76. EXPERIENCE\n",
            " 77. EVER\n",
            " 78. I\n",
            " 79. ALL\n",
            " 80. I\n",
            " 81. BARGAIN\n",
            " 82. I\n",
            " 83. TV\n",
            " 84. NONE\n",
            " 85. I\n",
            " 86. M\n",
            " 87. FREEZING\n",
            " 88. AYCE\n",
            " 89. I\n",
            " 90. I\n",
            " 91. I\n",
            " 92. I\n",
            " 93. I\n",
            " 94. I\n",
            " 95. I\n",
            " 96. I\n",
            " 97. I\n",
            " 98. I\n",
            " 99. I\n",
            "100. I\n",
            "101. I\n",
            "102. FLAVOR\n",
            "103. I\n",
            "104. I\n",
            "105. I\n",
            "106. I\n",
            "107. I\n",
            "108. I\n",
            "109. I\n",
            "110. I\n",
            "111. I\n",
            "112. I\n",
            "113. I\n",
            "114. I\n",
            "115. I\n",
            "116. I\n",
            "117. I\n",
            "118. I\n",
            "119. I\n",
            "120. I\n",
            "121. NEVER\n",
            "122. I\n",
            "123. I\n",
            "124. I\n",
            "125. I\n",
            "126. BBQ\n",
            "127. I\n",
            "128. A\n",
            "129. I\n",
            "130. UNREAL\n",
            "131. I\n",
            "132. I\n",
            "133. I\n",
            "134. I\n",
            "135. I\n",
            "136. I\n",
            "137. OMG\n",
            "138. I\n",
            "139. BETTER\n",
            "140. I\n",
            "141. I\n",
            "142. BLAND\n",
            "143. I\n",
            "144. I\n",
            "145. I\n",
            "146. I\n",
            "147. I\n",
            "148. I\n",
            "149. I\n",
            "150. RUDE\n",
            "151. INCONSIDERATE\n",
            "152. MANAGEMENT\n",
            "153. I\n",
            "154. I\n",
            "155. WILL\n",
            "156. NEVER\n",
            "157. EVER\n",
            "158. GO\n",
            "159. BACK\n",
            "160. AND\n",
            "161. HAVE\n",
            "162. TOLD\n",
            "163. MANY\n",
            "164. PEOPLE\n",
            "165. WHAT\n",
            "166. HAD\n",
            "167. HAPPENED\n",
            "168. I\n",
            "169. I\n",
            "170. I\n",
            "171. I\n",
            "172. I\n",
            "173. I\n",
            "174. I\n",
            "175. I\n",
            "176. I\n",
            "177. I\n",
            "178. I\n",
            "179. I\n",
            "180. I\n",
            "181. TOTAL\n",
            "182. WASTE\n",
            "183. OF\n",
            "184. TIME\n",
            "185. I\n",
            "186. I\n",
            "187. I\n",
            "188. I\n",
            "189. I\n",
            "190. I\n",
            "191. I\n",
            "192. I\n",
            "193. I\n",
            "194. I\n",
            "195. I\n",
            "196. I\n",
            "197. I\n",
            "198. I\n",
            "199. I\n",
            "200. FS\n",
            "201. I\n",
            "202. I\n",
            "203. I\n",
            "204. I\n",
            "205. I\n",
            "206. I\n",
            "207. I\n",
            "208. I\n",
            "209. I\n",
            "210. AZ\n",
            "211. I\n",
            "212. I\n",
            "213. I\n",
            "214. I\n",
            "215. LOVED\n",
            "216. I\n",
            "217. I\n",
            "218. I\n",
            "219. I\n",
            "220. I\n",
            "221. I\n",
            "222. CONCLUSION\n",
            "223. I\n",
            "224. I\n",
            "225. I\n",
            "226. I\n",
            "227. I\n",
            "228. I\n",
            "229. BEST\n",
            "230. I\n",
            "231. GO\n",
            "232. NOW\n",
            "233. A\n",
            "234. I\n",
            "235. I\n",
            "236. GC\n",
            "237. I\n",
            "238. I\n",
            "239. I\n",
            "240. I\n",
            "241. I\n",
            "242. I\n",
            "243. I\n",
            "244. I\n",
            "245. I\n",
            "246. I\n",
            "247. I\n",
            "248. AVOID\n",
            "249. THIS\n",
            "250. ESTABLISHMENT\n",
            "251. I\n",
            "252. I\n",
            "253. I\n",
            "254. A\n",
            "255. I\n",
            "256. I\n",
            "257. AN\n",
            "258. HOUR\n",
            "259. I\n",
            "260. I\n",
            "261. I\n",
            "262. I\n",
            "263. I\n",
            "264. I\n",
            "265. I\n",
            "266. I\n",
            "267. NASTY\n",
            "268. OMG\n",
            "269. I\n",
            "270. I\n",
            "271. NO\n",
            "272. I\n",
            "273. I\n",
            "274. I\n",
            "275. BEST\n",
            "276. I\n",
            "277. I\n",
            "278. I\n",
            "279. I\n",
            "280. THE\n",
            "281. OWNERS\n",
            "282. REALLY\n",
            "283. REALLY\n",
            "284. I\n",
            "285. I\n",
            "286. I\n",
            "287. I\n",
            "288. I\n",
            "289. I\n",
            "290. I\n",
            "291. I\n",
            "292. I\n",
            "293. I\n",
            "294. I\n",
            "295. I\n",
            "296. PERFECT\n",
            "297. SCREAMS\n",
            "298. LEGIT\n",
            "299. I\n",
            "300. MGM\n",
            "301. I\n",
            "302. I\n",
            "303. I\n",
            "304. A\n",
            "305. I\n",
            "306. I\n",
            "307. I\n",
            "308. I\n",
            "309. I\n",
            "310. I\n",
            "311. I\n",
            "312. I\n",
            "313. I\n",
            "314. I\n",
            "315. I\n",
            "316. I\n",
            "317. BEST\n",
            "318. I\n",
            "319. I\n",
            "320. T\n",
            "321. A\n",
            "322. FLY\n",
            "323. A\n",
            "324. FLY\n",
            "325. I\n",
            "326. I\n",
            "327. I\n",
            "328. I\n",
            "329. I\n",
            "330. I\n",
            "331. I\n",
            "332. I\n",
            "333. I\n",
            "334. I\n",
            "335. I\n",
            "336. I\n",
            "337. FANTASTIC\n",
            "338. I\n",
            "339. I\n",
            "340. I\n",
            "341. I\n",
            "342. GREAT\n",
            "343. OK\n",
            "344. I\n",
            "345. I\n",
            "346. WAY\n",
            "347. I\n",
            "348. I\n",
            "349. MUST\n",
            "350. HAVE\n",
            "351. I\n",
            "352. I\n",
            "353. I\n",
            "354. I\n",
            "355. I\n",
            "356. I\n",
            "357. OK\n",
            "358. I\n",
            "359. I\n",
            "360. I\n",
            "361. I\n",
            "362. OVERPRICED\n",
            "363. I\n",
            "364. I\n",
            "365. I\n",
            "366. I\n",
            "367. I\n",
            "368. I\n",
            "369. I\n",
            "370. I\n",
            "371. I\n",
            "372. I\n",
            "373. I\n",
            "374. I\n",
            "375. I\n",
            "376. I\n",
            "377. I\n",
            "378. I\n",
            "379. I\n",
            "380. I\n",
            "381. I\n",
            "382. I\n",
            "383. I\n",
            "384. I\n",
            "385. I\n",
            "386. I\n",
            "387. I\n",
            "388. BARE\n",
            "389. HANDS\n",
            "390. I\n",
            "391. I\n",
            "392. I\n",
            "393. I\n",
            "394. I\n",
            "395. I\n",
            "396. I\n",
            "397. I\n",
            "398. I\n",
            "399. I\n",
            "400. I\n",
            "401. I\n",
            "402. I\n",
            "403. I\n",
            "404. I\n",
            "405. WEAK\n",
            "406. I\n",
            "407. I\n",
            "408. I\n",
            "409. I\n",
            "410. I\n",
            "411. I\n",
            "412. I\n",
            "413. I\n",
            "414. SHOULD\n",
            "415. I\n",
            "416. I\n",
            "417. I\n",
            "418. RI\n",
            "419. I\n",
            "420. I\n",
            "421. I\n",
            "422. I\n",
            "423. I\n",
            "424. I\n",
            "425. I\n",
            "426. VERY\n",
            "427. I\n",
            "428. I\n",
            "429. I\n",
            "430. NOT\n",
            "431. I\n",
            "432. I\n",
            "433. I\n",
            "434. I\n",
            "435. I\n",
            "436. I\n",
            "437. I\n",
            "438. I\n",
            "439. I\n",
            "440. I\n",
            "441. I\n",
            "442. I\n",
            "443. I\n",
            "444. I\n",
            "445. I\n",
            "446. I\n",
            "447. I\n",
            "448. I\n",
            "449. A\n",
            "450. I\n",
            "451. I\n",
            "452. I\n",
            "453. I\n",
            "454. I\n",
            "455. I\n",
            "Total number of strings found: 455\n"
          ]
        }
      ],
      "source": [
        "Q = []\n",
        "for doc in docs:\n",
        "  tmp = re.findall(r'\\b[A-Z]+\\b', doc)  \n",
        "  if tmp:\n",
        "    Q = Q + tmp\n",
        "\n",
        "print('First 10 comments from the result:')\n",
        "print_elements(Q)\n",
        "print('Total number of strings found:', len(Q))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p>If we want to see only the unique words that appear, we can do the following:</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 10 comments from the result:\n",
            " 1. UNREAL\n",
            " 2. FANTASTIC\n",
            " 3. AYCE\n",
            " 4. VERY\n",
            " 5. MUST\n",
            " 6. RI\n",
            " 7. ALL\n",
            " 8. PEOPLE\n",
            " 9. MGM\n",
            "10. REAL\n",
            "11. STALE\n",
            "12. AN\n",
            "13. IN\n",
            "14. THAT\n",
            "15. APPETIZERS\n",
            "16. OWNERS\n",
            "17. OF\n",
            "18. A\n",
            "19. AVOID\n",
            "20. MANAGEMENT\n",
            "21. NOT\n",
            "22. BLAND\n",
            "23. NYC\n",
            "24. HAPPENED\n",
            "25. NOW\n",
            "26. HAVE\n",
            "27. OK\n",
            "28. REALLY\n",
            "29. WHAT\n",
            "30. OMG\n",
            "31. MANY\n",
            "32. ESTABLISHMENT\n",
            "33. BBQ\n",
            "34. GO\n",
            "35. BACK\n",
            "36. GREAT\n",
            "37. INCONSIDERATE\n",
            "38. AGAIN\n",
            "39. TOTAL\n",
            "40. SCREAMS\n",
            "41. BITCHES\n",
            "42. I\n",
            "43. DELICIOUS\n",
            "44. SHOULD\n",
            "45. BETTER\n",
            "46. WILL\n",
            "47. FLY\n",
            "48. RUDE\n",
            "49. TV\n",
            "50. IT\n",
            "51. AND\n",
            "52. LEGIT\n",
            "53. FORWARD\n",
            "54. TIME\n",
            "55. FS\n",
            "56. EVER\n",
            "57. NONE\n",
            "58. HAD\n",
            "59. NEVER\n",
            "60. FREEZING\n",
            "61. STEP\n",
            "62. TOLD\n",
            "63. THE\n",
            "64. GC\n",
            "65. WASTE\n",
            "66. WORST\n",
            "67. WEAK\n",
            "68. FLAVOR\n",
            "69. BEST\n",
            "70. LOVED\n",
            "71. WAY\n",
            "72. HOUR\n",
            "73. PERFECT\n",
            "74. BARGAIN\n",
            "75. HANDS\n",
            "76. AZ\n",
            "77. M\n",
            "78. NASTY\n",
            "79. T\n",
            "80. BARE\n",
            "81. EXPERIENCE\n",
            "82. THIS\n",
            "83. OVERPRICED\n",
            "84. CONCLUSION\n",
            "85. NO\n",
            "Total number of unique strings found: 85\n"
          ]
        }
      ],
      "source": [
        "Q = []\n",
        "for doc in docs:\n",
        "  tmp = re.findall(r'\\b[A-Z]+\\b', doc)  \n",
        "  if tmp:\n",
        "    Q = Q + tmp\n",
        "\n",
        "print('First 10 comments from the result:')\n",
        "print_elements(set(Q))\n",
        "print('Total number of unique strings found:', len(set(Q)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h2>Problem 4</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GX8eYyDoMZma"
      },
      "source": [
        "\n",
        "Search for and print comments where all alphabetical characters (letters) are in uppercase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "K8VuZxvTMYj6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 10 comments from the result:\n",
            "1. DELICIOUS!!\n",
            "\n",
            "2. RUDE & INCONSIDERATE MANAGEMENT.\n",
            "\n",
            "3. WILL NEVER EVER GO BACK AND HAVE TOLD MANY PEOPLE WHAT HAD HAPPENED.\n",
            "\n",
            "4. TOTAL WASTE OF TIME.\n",
            "\n",
            "5. AVOID THIS ESTABLISHMENT!\n",
            "\n",
            "Total number of strings found: 5\n"
          ]
        }
      ],
      "source": [
        "Q = []\n",
        "for doc in docs:\n",
        "  tmp = re.findall(r'^[^a-z]*$', doc)  # Find lines with no lowercase letters\n",
        "  if tmp:\n",
        "    Q = Q + tmp\n",
        "\n",
        "print('First 10 comments from the result:')\n",
        "print_elements(Q)\n",
        "print('Total number of strings found:', len(Q))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h2>Problem 5</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1i6qv7-McmU"
      },
      "source": [
        "Search for and print all words that contain an accented vowel, such as á, é, í, ó, ú."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "nZZ5zKUOMeGD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 10 comments from the result:\n",
            "1. My fiancé and I came in the middle of the day and we were greeted and seated right away\n",
            "2. I really enjoyed Crema Café before they expanded; I even told friends they had the BEST breakfast\n",
            "3. The only thing I wasn't too crazy about was their guacamole as I don't like it puréed\n",
            "Total number of strings found: 3\n"
          ]
        }
      ],
      "source": [
        "Q = []\n",
        "for doc in docs:\n",
        "  tmp = re.findall(r'\\b.*[áéíóú].*\\b', doc)  # Find words with accented vowels\n",
        "  if tmp:\n",
        "    Q = Q + tmp\n",
        "\n",
        "print('First 10 comments from the result:')\n",
        "print_elements(Q)\n",
        "print('Total number of strings found:', len(Q))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h2>Problem 6</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmPiAI82Mfb3"
      },
      "source": [
        "\n",
        "Search for and print all monetary amounts, either whole or with decimals, that start with the dollar sign ($).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "6vhe9-Y-MhL9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 10 comments from the result:\n",
            "1. $20\n",
            "2. $4.00\n",
            "3. $17\n",
            "4. $3\n",
            "5. $35\n",
            "6. $7.85\n",
            "7. $12\n",
            "8. $11.99\n",
            "Total number of monetary strings found: 8\n"
          ]
        }
      ],
      "source": [
        "Q = []\n",
        "for doc in docs:\n",
        "  tmp = re.findall(r'\\$[0-9]*\\.?[0-9]+', doc)  # Find monetary amounts starting with $\n",
        "  if tmp:\n",
        "    Q = Q + tmp\n",
        "\n",
        "print('First 10 comments from the result:')\n",
        "print_elements(Q)\n",
        "print('Total number of monetary strings found:', len(Q))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h2>Problem 7</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2j-HpvhwMhq2"
      },
      "source": [
        "\n",
        "Search for and print all words that are variants of the word \"love,\" regardless of whether they include upper or lower case letters, or how they are conjugated or any other variation of the word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "kqqyRChVMjol"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 10 comments from the result:\n",
            " 1. Loved\n",
            " 2. loved\n",
            " 3. Loved\n",
            " 4. love\n",
            " 5. loves\n",
            " 6. LOVED\n",
            " 7. lovers\n",
            " 8. loving\n",
            " 9. love\n",
            "10. lovers\n",
            "11. Love\n",
            "12. loved\n",
            "13. loved\n",
            "14. love\n",
            "15. love\n",
            "16. love\n",
            "17. loved\n",
            "18. love\n",
            "19. loved\n",
            "20. Love\n",
            "21. LOVED\n",
            "22. love\n",
            "23. lovely\n",
            "24. love\n",
            "25. lovely\n",
            "26. love\n",
            "27. lover\n",
            "28. loved\n",
            "29. love\n",
            "30. love\n",
            "31. love\n",
            "32. love\n",
            "33. gloves\n",
            "34. love\n",
            "35. love\n",
            "36. love\n",
            "37. love\n",
            "Total number of strings found: 37\n"
          ]
        }
      ],
      "source": [
        "Q = []\n",
        "for doc in docs:\n",
        "  tmp = re.findall(r'\\b\\w*lov\\w*\\b', doc, flags=re.IGNORECASE)  # Find all variations of \"love\"\n",
        "  if tmp:\n",
        "    Q = Q + tmp\n",
        "\n",
        "print('First 10 comments from the result:')\n",
        "print_elements(Q)\n",
        "print('Total number of strings found:', len(Q))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h2>Problem 8</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ctb-NTY3MkYG"
      },
      "source": [
        "\n",
        "Search for and print all words that are variants of \"so\" and \"good\", which have two or more \"o\"s in \"so\" and three or more \"o\"s in \"good\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "A8Nf3B_cMlqg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 10 comments from the result:\n",
            "1. Sooooo\n",
            "2. soooo\n",
            "3. gooodd\n",
            "4. soooooo\n",
            "5. soooo\n",
            "Total number of strings found: 5\n"
          ]
        }
      ],
      "source": [
        "Q = []\n",
        "for doc in docs:\n",
        "  # Find words that are variants of \"so\" with two or more 'o's and \"good\" with three or more 'o's\n",
        "  tmp = re.findall(r'\\b(s+o{2,})\\b|\\b(g+o{3,}d+)\\b', doc, flags=re.IGNORECASE)  \n",
        "  if tmp:\n",
        "    # Flatten the tuple results and remove empty strings\n",
        "    Q += [element for tuple_elements in tmp for element in tuple_elements if element]\n",
        "\n",
        "print('First 10 comments from the result:')\n",
        "print_elements(Q)\n",
        "print('Total number of strings found:', len(Q))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h2>Problem 9</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkak1opjMmlk"
      },
      "source": [
        "\n",
        "Search for and print all words that have a length strictly greater than 10 alphabetic characters. Punctuation marks or special characters are not considered in the length of these strings, only uppercase or lowercase alphabetic characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "PYxdp3uhMoD0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 10 comments from the result:\n",
            "  1. recommendation\n",
            "  2. recommended\n",
            "  3. overwhelmed\n",
            "  4. inexpensive\n",
            "  5. establishment\n",
            "  6. imaginative\n",
            "  7. opportunity\n",
            "  8. experiencing\n",
            "  9. underwhelming\n",
            " 10. relationship\n",
            " 11. unsatisfying\n",
            " 12. disappointing\n",
            " 13. outrageously\n",
            " 14. disappointing\n",
            " 15. expectations\n",
            " 16. restaurants\n",
            " 17. suggestions\n",
            " 18. disappointed\n",
            " 19. considering\n",
            " 20. Unfortunately\n",
            " 21. immediately\n",
            " 22. ingredients\n",
            " 23. accommodations\n",
            " 24. maintaining\n",
            " 25. Interesting\n",
            " 26. disrespected\n",
            " 27. accordingly\n",
            " 28. unbelievable\n",
            " 29. cheeseburger\n",
            " 30. descriptions\n",
            " 31. inexpensive\n",
            " 32. disappointed\n",
            " 33. Veggitarian\n",
            " 34. outstanding\n",
            " 35. recommendation\n",
            " 36. disappointed\n",
            " 37. disappointed\n",
            " 38. neighborhood\n",
            " 39. disappointed\n",
            " 40. corporation\n",
            " 41. considering\n",
            " 42. exceptional\n",
            " 43. shawarrrrrrma\n",
            " 44. disappointed\n",
            " 45. vinaigrette\n",
            " 46. immediately\n",
            " 47. unbelievably\n",
            " 48. replenished\n",
            " 49. disappointed\n",
            " 50. enthusiastic\n",
            " 51. Outstanding\n",
            " 52. comfortable\n",
            " 53. interesting\n",
            " 54. INCONSIDERATE\n",
            " 55. considering\n",
            " 56. transcendant\n",
            " 57. disappointment\n",
            " 58. disappointed\n",
            " 59. disappointed\n",
            " 60. overwhelmed\n",
            " 61. professional\n",
            " 62. Furthermore\n",
            " 63. combination\n",
            " 64. connoisseur\n",
            " 65. profiterole\n",
            " 66. outstanding\n",
            " 67. acknowledged\n",
            " 68. ventilation\n",
            " 69. beautifully\n",
            " 70. establishment\n",
            " 71. extraordinary\n",
            " 72. disappointed\n",
            " 73. cheesecurds\n",
            " 74. disappointed\n",
            " 75. interesting\n",
            " 76. experienced\n",
            " 77. opportunity\n",
            " 78. disgraceful\n",
            " 79. restaurants\n",
            " 80. ESTABLISHMENT\n",
            " 81. recommended\n",
            " 82. disappointed\n",
            " 83. recommended\n",
            " 84. acknowledged\n",
            " 85. presentation\n",
            " 86. Philadelphia\n",
            " 87. disappointed\n",
            " 88. disappointing\n",
            " 89. grandmother\n",
            " 90. drastically\n",
            " 91. informative\n",
            " 92. Disappointed\n",
            " 93. constructed\n",
            " 94. comfortable\n",
            " 95. Smashburger\n",
            " 96. cheeseburger\n",
            " 97. neighborhood\n",
            " 98. disappointed\n",
            " 99. hospitality\n",
            "100. recommending\n",
            "101. disappointed\n",
            "102. deliciously\n",
            "103. compliments\n",
            "104. recommendation\n",
            "105. establishment\n",
            "106. calligraphy\n",
            "107. traditional\n",
            "108. combination\n",
            "109. Unfortunately\n",
            "110. Wienerschnitzel\n",
            "111. unfortunately\n",
            "112. considering\n",
            "113. highlighted\n",
            "114. Mediterranean\n",
            "115. unprofessional\n",
            "116. anticipated\n",
            "117. disappointing\n",
            "118. unexperienced\n",
            "119. disrespected\n",
            "120. professional\n",
            "121. restaurants\n",
            "122. Disappointing\n",
            "123. WAAAAAAyyyyyyyyyy\n",
            "124. reservation\n",
            "125. imagination\n",
            "126. undercooked\n",
            "127. disappointed\n",
            "128. disappointment\n",
            "129. disappointment\n",
            "130. deuchebaggery\n",
            "131. disappointed\n",
            "132. disappointment\n",
            "133. immediately\n",
            "134. Unfortunately\n",
            "135. disapppointment\n",
            "136. circumstances\n",
            "137. undercooked\n",
            "138. caterpillar\n",
            "139. presentation\n",
            "140. disappointed\n",
            "141. underwhelming\n",
            "Total number of strings found: 141\n"
          ]
        }
      ],
      "source": [
        "Q = []\n",
        "for doc in docs:\n",
        "  # Find words that have more than 10 alphabetic characters\n",
        "  tmp = re.findall(r'\\b[a-z]{11,}\\b', doc, flags=re.IGNORECASE)  \n",
        "  if tmp:\n",
        "    Q += tmp\n",
        "\n",
        "print('First 10 comments from the result:')\n",
        "print_elements(Q)\n",
        "print('Total number of strings found:', len(Q))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h2>Problem 10</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApjTNzSxMpDc"
      },
      "source": [
        "Search for and print all words that start with an uppercase letter and end with a lowercase letter, but are not the first word of the comment/string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Vb0ndRGAMqdL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 10 comments from the result:\n",
            "  1. Loved\n",
            "  2. May\n",
            "  3. Rick\n",
            "  4. Steve\n",
            "  5. Cape\n",
            "  6. Cod\n",
            "  7. Vegas\n",
            "  8. Burrittos\n",
            "  9. Blah\n",
            " 10. The\n",
            " 11. They\n",
            " 12. Mexican\n",
            " 13. Luke\n",
            " 14. Our\n",
            " 15. Hiro\n",
            " 16. Firehouse\n",
            " 17. Greek\n",
            " 18. Greek\n",
            " 19. Heart\n",
            " 20. Attack\n",
            " 21. Grill\n",
            " 22. Vegas\n",
            " 23. Dos\n",
            " 24. Gringos\n",
            " 25. Jeff\n",
            " 26. Really\n",
            " 27. Excalibur\n",
            " 28. Very\n",
            " 29. Bad\n",
            " 30. Customer\n",
            " 31. Service\n",
            " 32. Vegas\n",
            " 33. Rice\n",
            " 34. Company\n",
            " 35. Pho\n",
            " 36. Hard\n",
            " 37. Rock\n",
            " 38. Casino\n",
            " 39. Buffet\n",
            " 40. Tigerlilly\n",
            " 41. Yama\n",
            " 42. Thai\n",
            " 43. Indian\n",
            " 44. Not\n",
            " 45. Vegas\n",
            " 46. Lox\n",
            " 47. Subway\n",
            " 48. Subway\n",
            " 49. Vegas\n",
            " 50. Vegas\n",
            " 51. Mandalay\n",
            " 52. Bay\n",
            " 53. Great\n",
            " 54. Voodoo\n",
            " 55. Phoenix\n",
            " 56. Vegas\n",
            " 57. Khao\n",
            " 58. Soi\n",
            " 59. Lemon\n",
            " 60. Joey\n",
            " 61. Valley\n",
            " 62. Phoenix\n",
            " 63. Magazine\n",
            " 64. Pho\n",
            " 65. Fridays\n",
            " 66. Tasty\n",
            " 67. Jamaican\n",
            " 68. Bisque\n",
            " 69. Bussell\n",
            " 70. Sprouts\n",
            " 71. Risotto\n",
            " 72. Filet\n",
            " 73. Otto\n",
            " 74. Yeah\n",
            " 75. Honestly\n",
            " 76. Not\n",
            " 77. Also\n",
            " 78. Vegas\n",
            " 79. Greek\n",
            " 80. Vegas\n",
            " 81. Veggitarian\n",
            " 82. Madison\n",
            " 83. Ironman\n",
            " 84. Jenni\n",
            " 85. Pho\n",
            " 86. Bachi\n",
            " 87. Burger\n",
            " 88. Pizza\n",
            " 89. Salads\n",
            " 90. They\n",
            " 91. Yelpers\n",
            " 92. Bachi\n",
            " 93. Service\n",
            " 94. English\n",
            " 95. Pizza\n",
            " 96. Hut\n",
            " 97. Seat\n",
            " 98. Standard\n",
            " 99. Thai\n",
            "100. Tucson\n",
            "101. Vegas\n",
            "102. Chipotle\n",
            "103. Gordon\n",
            "104. Ramsey\n",
            "105. Steak\n",
            "106. Vegas\n",
            "107. Outstanding\n",
            "108. Best\n",
            "109. Food\n",
            "110. Lobster\n",
            "111. Bisque\n",
            "112. Vegas\n",
            "113. Eggplant\n",
            "114. Green\n",
            "115. Bean\n",
            "116. Halibut\n",
            "117. Vegas\n",
            "118. Vegas\n",
            "119. Vegas\n",
            "120. Crystals\n",
            "121. Aria\n",
            "122. Ians\n",
            "123. Bouchon\n",
            "124. San\n",
            "125. Francisco\n",
            "126. Bay\n",
            "127. Area\n",
            "128. Buldogis\n",
            "129. Gourmet\n",
            "130. Hot\n",
            "131. Dog\n",
            "132. Steiners\n",
            "133. Carly\n",
            "134. Vegas\n",
            "135. Camelback\n",
            "136. Flower\n",
            "137. Shop\n",
            "138. Cartel\n",
            "139. Coffee\n",
            "140. Las\n",
            "141. Vegas\n",
            "142. Very\n",
            "143. Mom\n",
            "144. Noca\n",
            "145. Vegas\n",
            "146. Sat\n",
            "147. Sun\n",
            "148. Mexican\n",
            "149. Frenchman\n",
            "150. Perfect\n",
            "151. Vegas\n",
            "152. Palm\n",
            "153. This\n",
            "154. Thai\n",
            "155. Toast\n",
            "156. Thai\n",
            "157. Phoenix\n",
            "158. Crema\n",
            "159. Philadelphia\n",
            "160. North\n",
            "161. Scottsdale\n",
            "162. Bloody\n",
            "163. Mary\n",
            "164. Pho\n",
            "165. Caesar\n",
            "166. Macarons\n",
            "167. Experience\n",
            "168. Very\n",
            "169. Disappointed\n",
            "170. Big\n",
            "171. Bay\n",
            "172. Plater\n",
            "173. Italian\n",
            "174. Vegas\n",
            "175. Baba\n",
            "176. Ganoush\n",
            "177. Nobu\n",
            "178. Smashburger\n",
            "179. Panna\n",
            "180. Cotta\n",
            "181. Breeze\n",
            "182. Mango\n",
            "183. Magic\n",
            "184. Pineapple\n",
            "185. Delight\n",
            "186. The\n",
            "187. Strip\n",
            "188. Steak\n",
            "189. Paradise\n",
            "190. Valley\n",
            "191. Cibo\n",
            "192. Thumbs\n",
            "193. Up\n",
            "194. Italian\n",
            "195. Pros\n",
            "196. Large\n",
            "197. Nice\n",
            "198. Great\n",
            "199. The\n",
            "200. Elk\n",
            "201. Filet\n",
            "202. Dylan\n",
            "203. All\n",
            "204. Han\n",
            "205. Nan\n",
            "206. Chicken\n",
            "207. Bar\n",
            "208. Edinburgh\n",
            "209. Chinese\n",
            "210. Indian\n",
            "211. Chinese\n",
            "212. Prices\n",
            "213. Phoenix\n",
            "214. Both\n",
            "215. Hot\n",
            "216. Sour\n",
            "217. Egg\n",
            "218. Flower\n",
            "219. Soups\n",
            "220. Stars\n",
            "221. Sunday\n",
            "222. Hunan\n",
            "223. The\n",
            "224. Pita\n",
            "225. Wienerschnitzel\n",
            "226. Maine\n",
            "227. Lobster\n",
            "228. Roll\n",
            "229. Kabuki\n",
            "230. Maria\n",
            "231. Caballero\n",
            "232. Wife\n",
            "233. Strip\n",
            "234. Costco\n",
            "235. To\n",
            "236. Place\n",
            "237. Gyros\n",
            "238. Japanese\n",
            "239. Albondigas\n",
            "240. Mediterranean\n",
            "241. Chicken\n",
            "242. Salad\n",
            "243. Mellow\n",
            "244. Mushroom\n",
            "245. Thai\n",
            "246. Vegas\n",
            "247. Mmmm\n",
            "248. Buffet\n",
            "249. Bellagio\n",
            "250. Vegas\n",
            "251. Christmas\n",
            "252. Eve\n",
            "253. Denny\n",
            "254. Vegetarian\n",
            "255. Taco\n",
            "256. Heimer\n",
            "257. Ha\n",
            "258. Long\n",
            "259. Bay\n",
            "260. Subway\n",
            "261. When\n",
            "262. Brushfire\n",
            "263. Mirage\n",
            "264. In\n",
            "265. Ninja\n",
            "266. Sushi\n",
            "Total number of strings found: 266\n"
          ]
        }
      ],
      "source": [
        "Q = []\n",
        "for doc in docs:\n",
        "  # Find words that start with an uppercase letter and end with a lowercase letter, \n",
        "  # and are not the first word in the string\n",
        "  tmp = re.findall(r'\\s([A-Z]\\w*[a-z])\\b', doc)  \n",
        "  if tmp:\n",
        "    Q += tmp\n",
        "\n",
        "print('First 10 comments from the result:')\n",
        "print_elements(Q)\n",
        "print('Total number of strings found:', len(Q))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h2>Problem 11</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7nfm4KhMrNW"
      },
      "source": [
        "Search and print the sequence of two or more words separated by a hyphen, \"-\", without any spaces between them.\n",
        "\n",
        "For example, \"Go-Kart\" would be valid, but \"Go -Kart\" or \"Go - Kart\" would not be."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "OwU-a7eGMsub"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 10 comments from the result:\n",
            "Total strings found: 0\n"
          ]
        }
      ],
      "source": [
        "Q = []\n",
        "for doc in docs:\n",
        "  tmp = re.findall(r'\\b(\\w+[-]\\w+)\\b', doc, flags=re.IGNORECASE)\n",
        "if tmp:\n",
        "  Q += tmp\n",
        "print('First 10 comments from the result:')\n",
        "print_elements(Q[:10])\n",
        "print('Total strings found:', len(Q))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h2>Problem 12</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEIgl79HMthr"
      },
      "source": [
        "\n",
        "Search and print all words ending in \"ing\" or \"ed\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "I4TSofBMMv9y"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 10 comments from the result:\n",
            " 1. during\n",
            " 2. getting\n",
            " 3. being\n",
            " 4. being\n",
            " 5. amazing\n",
            " 6. running\n",
            " 7. redeeming\n",
            " 8. getting\n",
            " 9. thing\n",
            "10. dressing\n",
            "11. Loved\n",
            "12. Stopped\n",
            "13. loved\n",
            "14. ended\n",
            "15. overpriced\n",
            "16. tried\n",
            "17. disgusted\n",
            "18. shocked\n",
            "19. recommended\n",
            "20. performed\n",
            "Total strings ending with \"ing\": 280\n",
            "Total strings ending with \"ed\": 339\n"
          ]
        }
      ],
      "source": [
        "ings = []\n",
        "eds = []\n",
        "for doc in docs:\n",
        "    ing = re.findall(r'\\b\\w+ing\\b', doc, flags=re.IGNORECASE)\n",
        "    ed = re.findall(r'\\b\\w+ed\\b', doc, flags=re.IGNORECASE)\n",
        "    if ing:\n",
        "        ings += ing\n",
        "    if ed:\n",
        "        eds += ed\n",
        "\n",
        "print('First 10 comments from the result:')\n",
        "print_elements(ings[:10] + eds[:10])\n",
        "print('Total strings ending with \"ing\":', len(ings))\n",
        "print('Total strings ending with \"ed\":', len(eds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70StdqAZa9E9"
      },
      "source": [
        "<h2>Corpus Preprocessing</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h2>Problem 13</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaDUFXHrMvX2"
      },
      "source": [
        "Now perform a cleaning process on the corpus that includes the following steps:\n",
        "\n",
        "<ul>\n",
        "<li>Only consider alphabetical characters. This means removing all punctuation marks and special characters.</li>\n",
        "<li>Transform all alphabetical characters to lowercase.</li>\n",
        "<li>Remove any additional white spaces found in each comment.</li>\n",
        "</ul>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "mYEDlHSFMyJN"
      },
      "outputs": [],
      "source": [
        "def clean_and_find_long_words(text):\n",
        "    \"\"\"\n",
        "    Cleans the text by removing non-alphabetic characters, converting to lowercase,\n",
        "    removing extra spaces, and then finds words with more than 10 alphabetic characters.\n",
        "    \n",
        "    Args:\n",
        "        text (str): The original text to process.\n",
        "    \n",
        "    Returns:\n",
        "        str: Cleaned text containing words with more than 10 alphabetic characters.\n",
        "    \"\"\"\n",
        "    # Remove non-alphabetic characters and convert to lowercase\n",
        "    cleaned_text = re.sub(r'[^a-zA-Z\\s]', '', text).lower()\n",
        "    \n",
        "    # Remove extra white spaces\n",
        "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
        "    \n",
        "    return cleaned_text    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "K3kQzPOPMx0w"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 10 comments from the result:\n",
            " 1. wow loved this place\n",
            " 2. crust is not good\n",
            " 3. not tasty and the texture was just nasty\n",
            " 4. stopped by during the late may bank holiday off rick steve recommendation and loved it\n",
            " 5. the selection on the menu was great and so were the prices\n",
            " 6. now i am getting angry and i want my damn pho\n",
            " 7. honeslty it didnt taste that fresh\n",
            " 8. the potatoes were like rubber and you could tell they had been made up ahead of time being kept under a warmer\n",
            " 9. the fries were great too\n",
            "10. a great touch\n"
          ]
        }
      ],
      "source": [
        "clean_data = []\n",
        "for doc in docs:\n",
        "    tmp = clean_and_find_long_words(doc)\n",
        "    if tmp:\n",
        "        clean_data.append(tmp)\n",
        "print('First 10 comments from the result:')\n",
        "print_elements(clean_data[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h2>Problem 14</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZwEhg2lUSAX"
      },
      "source": [
        "<ul>\n",
        "    <li>\n",
        "        With the cleaning result obtained in the previous question, now perform a word tokenization process on the corpus.\n",
        "        <ul>\n",
        "            <li>\n",
        "                That is, at the end of this tokenization process, you should have a list of lists where each comment will be tokenized by words.\n",
        "            </li>\n",
        "        </ul>\n",
        "    </li>\n",
        "    <li>\n",
        "        After finishing, calculate the total number of tokens obtained in the entire corpus.\n",
        "    </li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "kbAL9-v0V-jx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 10 comments from the result:\n",
            " 1. ['wow', 'loved', 'this', 'place']\n",
            " 2. ['crust', 'is', 'not', 'good']\n",
            " 3. ['not', 'tasty', 'and', 'the', 'texture', 'was', 'just', 'nasty']\n",
            " 4. ['stopped', 'by', 'during', 'the', 'late', 'may', 'bank', 'holiday', 'off', 'rick', 'steve', 'recommendation', 'and', 'loved', 'it']\n",
            " 5. ['the', 'selection', 'on', 'the', 'menu', 'was', 'great', 'and', 'so', 'were', 'the', 'prices']\n",
            " 6. ['now', 'i', 'am', 'getting', 'angry', 'and', 'i', 'want', 'my', 'damn', 'pho']\n",
            " 7. ['honeslty', 'it', 'didnt', 'taste', 'that', 'fresh']\n",
            " 8. ['the', 'potatoes', 'were', 'like', 'rubber', 'and', 'you', 'could', 'tell', 'they', 'had', 'been', 'made', 'up', 'ahead', 'of', 'time', 'being', 'kept', 'under', 'a', 'warmer']\n",
            " 9. ['the', 'fries', 'were', 'great', 'too']\n",
            "10. ['a', 'great', 'touch']\n",
            "Total tokens:  10777\n"
          ]
        }
      ],
      "source": [
        "tokenization = []\n",
        "num_tokens = 0\n",
        "for doc in clean_data:\n",
        "    tmp = re.findall(r'\\b\\w+\\b', doc, flags=re.IGNORECASE)  \n",
        "    if tmp:\n",
        "        tokenization.append(tmp)\n",
        "    num_tokens += len(tmp)\n",
        "print('First 10 comments from the result:')\n",
        "print_elements(tokenization[:10])\n",
        "print(\"Total tokens: \", num_tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h2>Problem 15</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFeu0OJ7WDPD"
      },
      "source": [
        "\n",
        "<ul>\n",
        "    <li>\n",
        "        Finally, in this exercise, we will define our set of stopwords, which you will need to remove from the entire corpus.\n",
        "        <ul>\n",
        "            <li>\n",
        "                Remember that examples of stopwords include articles, adverbs, conjunctions, etc., which have very high frequencies of occurrence in any document but do not provide much meaning in terms of the content of a statement.\n",
        "            </li>\n",
        "        </ul>\n",
        "    </li>\n",
        "    <li>\n",
        "        Based on the provided list of stopwords, perform a cleaning process by removing all these words from the corpus obtained in the previous exercise.\n",
        "    </li>\n",
        "    <li>\n",
        "        Determine how many tokens/words remain in the entire corpus.\n",
        "    </li>\n",
        "    <li>\n",
        "        Determine how many of these tokens/words are unique, that is, how many unique tokens our vocabulary will have later on.\n",
        "    </li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "6FP4FF3KXGxm"
      },
      "outputs": [],
      "source": [
        "# We consider the following as our list of stopwords:\n",
        "my_stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'should', 'now', 'll']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "CD8yjyq1ZrwY"
      },
      "outputs": [],
      "source": [
        "phrases_tokens = []\n",
        "for phrase in tokenization:\n",
        "    token_sin_stopword = []\n",
        "    for token in phrase:\n",
        "        if(token not in my_stop_words):\n",
        "            token_sin_stopword.append(token)\n",
        "    phrases_tokens.append(token_sin_stopword)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "4ZPi5prKZro5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens/words remaining in the entire corpus: 5776\n",
            "Unique tokens/words in the corpus: 1941\n"
          ]
        }
      ],
      "source": [
        "flat_phrases = list(np.concatenate(phrases_tokens))\n",
        "\n",
        "print(\"Tokens/words remaining in the entire corpus:\", len(flat_phrases))\n",
        "print(\"Unique tokens/words in the corpus:\", len(set(flat_phrases)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7fzbvqVbUGr"
      },
      "source": [
        "In practice, regular expressions are useful for various text preprocessing tasks such as lemmatization, stemming, text normalization, and calculating edit distances. These techniques streamline text preparation for Natural Language Processing (NLP) by reducing words to their root forms, standardizing formats, and correcting typographical errors. Furthermore, regex supports matching complex patterns, which is crucial for extracting specific information such as dates and email addresses from large datasets. This capability not only simplifies data cleaning but also enhances advanced search functionalities in various contexts, making regex a versatile and powerful tool in text analysis and processing.\n",
        "\n",
        "In conclusion, regular expressions are indispensable in the field of NLP due to their efficiency and precision in text manipulation and analysis. They allow greater adaptability to the specific needs of projects and contribute significantly to the effectiveness and accuracy of NLP systems. For these reasons, mastering regex is essential for anyone working in language technology, as it facilitates the implementation of more robust and optimized solutions."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "759SG4TyfbUn",
        "Zj-h4drXD-X9",
        "BY6yifxscfrx",
        "k_ewoagic5jc",
        "70StdqAZa9E9"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.8 ('vsc')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "e25c565d703623dfbb60cb8ab5f509eb161fc3c7d27dcad6d194dd598d06239b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
